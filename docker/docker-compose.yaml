services:
  nifi:
    image: apache/nifi:2.2.0
    container_name: nifi
    hostname: nifi
    volumes:
    # ðŸ”¹Main NiFi configuration (flow.json.gz, nifi.properties, etc.)":"
    - ..\data\nifi\conf:/opt/nifi/nifi-current/conf
    # ðŸ”¹ Repository mounts for full persistence
    # ðŸ”¹- ..\data\nifi\flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
    - ..\data\nifi\content_repository:/opt/nifi/nifi-current/content_repository
    - ..\data\nifi\provenance_repository:/opt/nifi/nifi-current/provenance_repository
    - ..\data\nifi\state:/opt/nifi/nifi-current/state
    depends_on:
      - minio
    environment:
      NIFI_WEB_HTTPS_PORT: "8443"
      SINGLE_USER_CREDENTIALS_USERNAME: admin
      SINGLE_USER_CREDENTIALS_PASSWORD: admin123445678
      TZ: "America/Sao_Paulo"
    ports:
      - 8443:8443     
    networks:
      - netlab
 
  minio:
    image: minio/minio:RELEASE.2025-02-28T09-55-16Z
    container_name: minio
    ports:
      - "10000:9000"
      - "10001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123445678
    command: server --console-address ":9001" /data
    volumes:
      - ..\data\minio:/data                                  # para ler arquivos locais com FileStreamSource
    networks:
      - netlab

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123445678
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ..\data\postgres:/var/lib/postgresql/data
    networks:
      - netlab

  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    image: spark-custom:3.5.1-hadoop3.3.6
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8000:8080"
    volumes:
      - ../data/spark/spark-jobs:/opt/spark-jobs
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080"
    networks:
      - netlab

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    image: spark-custom:3.5.1-hadoop3.3.6
    container_name: spark-worker
    depends_on:
      - spark-master
    volumes:
      - ../data/spark/spark-jobs:/opt/spark-jobs
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    networks:
      - netlab

  airflow:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    image: custom-airflow:latest
    container_name: airflow
    depends_on:
      - postgres
      - spark-master
      - spark-worker
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=j9wbwZSvmNMGSMr0KUAHOKNWyR7Jc1zzDWsKQd-8MgM=
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.www.security.DefaultAuthBackend
      - AIRFLOW__WEBSERVER__RBAC=True
    volumes:
      - ../data/airflow/dags:/opt/airflow/dags
      - ../data/spark/spark-jobs:/opt/spark-jobs
    ports:
      - "8080:8080"
      - "8081:8081"
    command: >
        bash -c "
          echo 'Waiting for PostgreSQL...' &&
          while ! nc -z postgres 5432; do sleep 2; done &&
          airflow db migrate &&
          airflow standalone    
        "        
    networks:
      - netlab

networks:
  netlab:
    driver: bridge

